# -*- coding: utf-8 -*-
"""EASI - iSOMA.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/128cHpJwYW0GUIBf1hY4qu5NowREVXyH_

# **The iSOMA**

---

# 1. **The principle**

* Self-Organizing Migrating Algorithm, a swarm-based intelligence optimization algorithm, works based on the interaction between individuals in the population according to a given rule to find an optimal solution to the given problem. The mechanism constituting the SOMA lies in how to select individuals as a leader and migrants, how migrants move to the leader, as well as how to update better individuals into the population and eliminate the bad one. These processes are performed under loops named migration loops. Then, through many migration loops, these solutions are becoming better and better than the initial. This section briefly describes the principle of SOMA, shaping the basis for the analysis of SOMA's strengths and weaknesses.

**Weakness of SOMA**

***Stopping Criteria:***

* As described in the original version, after each individual has completed his movement, the best position in this path is selected for comparison with the original. It is clear that to find a better position, the SOMA needs to call the cost function many times (known as function evaluation ($FE$), each time execution of the cost function is considered one $FE$).
				
* For example, with the standard setting of SOMA: $ Pathlength=3.0 $ and $ Step = 0.11 $, each traveling individual has 27 different positions on its jumping path, which means that the SOMA has to spend 27 $FEs$ to evaluate these positions to find the better one. Meanwhile, other algorithms only use one $FE$ to improve their candidate solutions such as DE, PSO, and ABC, or some algorithm-specific parameter-less optimization techniques. This characteristic causes the algorithm to soon face the stop condition of maximum function evaluations ($MaxFEs$) and the premature convergence scenario before it searches for detail in the exploitation phase at the end of the optimization process.
				
***Move on the Edge:***
				
* On the other side, in each variable of the optimization problem, $ PRTVector_j $ accepts only one of the two values of 0 and 1. Therefore, this variable will be updated with a multiple of $ Step $ if the value of $ PRTVector_j $ is 1 (and vice versa it will remain in its position). Geometrically, this means that traveling individuals will move on the intersection edges of hyperplanes created by pairs of sides of variables, as shown in presentation.


# 2. **Working**

**a. Cost Function:**
* First of all, we have to have the "cost function" that's the problem we want to solve. It's the given equation, such as Schwefel, Rosenbrock, Rastrigin, Griewank, and Ackley functions.

* In this explanation example, we will solve those functions, and they are defined as below:
"""

import numpy
from functions import *
import opfunu

#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
def Schwefel(x):
## Schwefel's function. VarMin, VarMax = -500   , 500
  m, n = numpy.shape(x)
  f = numpy.zeros(n)
  for j in range(0, n):
    f[j] = 418.982887*m-numpy.sum(x[0:m,j]*numpy.sin(numpy.sqrt(abs(x[0:m,j]))))
  return f
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
def Rosenbrock(x):
## Rosenbrock's valley. VarMin, VarMax = -2.048 , 2.048
  m, n = numpy.shape(x)
  f = numpy.zeros(n)
  for j in range(0,n):
    f[j] = numpy.sum((1.0-x[0:m,j])**2) \
         + numpy.sum((x[1:m,j]-x[0:m-1,j])**2)
  return f
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
def Rastrigin(x):
## Rastrigin's function. VarMin, VarMax = -5.12  , 5.12
  m, n = numpy.shape(x)
  f = numpy.zeros ( n )
  for j in range(0,n):
    f[j] = 10.0 * float(m)
    for i in range(0,m):
      f[j] = f[j]+x[i,j]**2-10.0*numpy.cos(2.0*numpy.pi*x[i,j])
  return f
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
def Griewank(x):
## Griewank's function. VarMin, VarMax = -600   , 600
  m, n = numpy.shape(x)
  f = numpy.zeros(n)
  y = list(range(1, m+1))
  y[0:m] = numpy.sqrt(y[0:m])
  for j in range(0,n):
    f[j] = numpy.sum(x[0:m,j]**2) / 4000.0 \
      - numpy.prod(numpy.cos(x[0:m,j] / y[0:m]))+1.0
  return f
#%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
def Ackley(x):
## Ackley's function. VarMin, VarMax = -1     , 1
  m, n = numpy.shape(x)
  f = numpy.zeros(n)
  a = 20.0
  b = 0.2
  c = 0.2
  for j in range(0, n):
    f[j] = - a * numpy.exp(-b*numpy.sqrt(numpy.sum(x[0:m,j]**2) / float(m))) \
      - numpy.exp(numpy.sum(numpy.cos(c*numpy.pi*x[0:m,j])) / float(m)) \
      + a + numpy.exp(1.0)
  return f

"""* For convenience, those mentioned functions above will be named as **CostFunction**
* We can change it like: CostFunction = Schwefel
"""

func = opfunu.get_functions_based_classname("2013")[1](ndim=30)
print(func.name)
CostFunction = lambda x: Wrapper(x, func.evaluate) 

"""**b. The iSOMA Algorithm:**
* In order for algorithms to work, we must define some parameters:
"""

dimension = 30                                                      # Number of dimensions of the problem
# -------------- Control Parameters of SOMA -------------------------------
N_jump, Step = 10, 0.3                                              # Assign values ​​to variables: Step, PRT, PathLength
PopSize, Max_Migration, Max_FEs = 100, 100, dimension*10**4         # Assign values ​​to variables: PopSize, Max_Migration
m, n, k = 10, 5, 15
# -------------- The domain (search space) --------------------------------
VarMin, VarMax = -func.bounds[0][1], func.bounds[0][1]   # for Schwefel's function.             # Define the search range

"""* At the beginning of the algorithm, a population is generated containing individuals as candidate solutions to a given problem:


"""

# %%%%%%%%%%%%%%      B E G I N    S O M A    %%%%%%%%%%%%%%%%%%%%%%%%%%%%%
# ------------- Create the initial Population -----------------------------
pop = VarMin + numpy.random.rand(dimension, PopSize) * (VarMax - VarMin) # Create the initial Population
fitness = CostFunction(pop)                                         # Evaluate the initial population
FEs = PopSize                                                       # Count the number of function evaluations
the_best_cost = min(fitness)                                        # Find the Global minimum fitness value
best_cost_old = the_best_cost

"""* The Main code of the iSOMA"""

# ---------------- SOMA MIGRATIONS ----------------------------------------
best_cost_old = the_best_cost
Migration, Count = 0, 0                                             # Assign values ​​to variables: Migration
while FEs < Max_FEs:                                                # Terminate when reaching Max_Migration / User can change to Max_FEs
    Migration = Migration + 1                                       # Increase Migration value
    # ------------ Migrant selection: m -----------------------------------
    M = numpy.random.choice(range(PopSize),m,replace=False)         # Migrant selection: m
    M_sort = numpy.argsort(fitness[M])
    for j in range(n):                                              # Choose n individuals move toward the Leader
        Migrant = pop[:, M[M_sort[j]]].reshape(dimension, 1)        # Get the Migrant position (solution values) in the current population
        # ------------ Leader selection: k --------------------------------
        K = numpy.random.choice(range(PopSize),k,replace=False)     # Leader selection: k
        K_sort = numpy.argsort(fitness[K])
        Leader = pop[:, K[K_sort[1]]].reshape(dimension, 1)         # Get the Migrant position (solution values) in the current population
        if M[M_sort[j]] == K[K_sort[1]]:                            # Don't move if it is itself
            Leader = pop[:, K[K_sort[2]]].reshape(dimension, 1)     # Get the Migrant position (solution values) in the current population
        # ------ Migrant move to Leader: Jumping --------------------------
        flag, move = 0, 1
        while (flag == 0) and (move <= N_jump):
            nstep = (N_jump-move+1) * Step
            # ------ Update Control parameters: PRT -----------------------
            PRT = 0.1 + 0.9*(FEs / Max_FEs);                        # Update PRT parameter
            # ----- SOMA Mutation -----------------------------------------
            PRTVector = (numpy.random.rand(dimension,1)<PRT)*1      # If rand() < PRT, PRTVector = 1, else, 0
            #PRTVector = (PRTVector - 1) * (1 - FEs/Max_FEs) + 1    # If rand() < PRT, PRTVector = 1, else, FEs/Max_FEs
            offspring = Migrant + (Leader - Migrant) * nstep * PRTVector # Jumping towards the Leader
            # ------------ Check and put individuals inside the search range if it's outside
            for rw in range(dimension):                             # From row: Check
                if offspring[rw]<VarMin or offspring[rw]>VarMax:    # if outside the search range
                    offspring[rw] = VarMin + numpy.random.rand() * (VarMax - VarMin) # Randomly put it inside
            # ------------ Evaluate the offspring and Update --------------
            new_cost = CostFunction(offspring)                      # Evaluate the offspring
            FEs = FEs + 1                                           # Count the number of function evaluations
            # ----- SOMA Accepting: Place the Best Offspring to Pop -------
            if new_cost <= fitness[M[M_sort[j]]]:                   # Compare min_new_cost with fitness value of the moving individual
                flag = 1
                fitness[M[M_sort[j]]] = new_cost                    # Replace the moving individual fitness value
                pop[:, [M[M_sort[j]]]] = offspring                  # Replace the moving individual position (solution values)
                if new_cost < the_best_cost:                        # Compare Current minimum fitness with Global minimum fitness
                    the_best_cost = new_cost                        # Update Global minimun fitness value
                    the_best_value = offspring                      # Update Global minimun position
                else:
                    Count = Count + 1
            move = move + 1
    if Count > PopSize*50:
        if the_best_cost == best_cost_old:
            rat = round(0.1*PopSize)
            pop_temp = VarMin + numpy.random.rand(dimension, rat)*(VarMax-VarMin)
            fit_temp = CostFunction(pop_temp)
            FEs = FEs + rat
            D = numpy.random.choice(range(PopSize),rat,replace=False)
            pop[:,D] = pop_temp
            fitness[D] = fit_temp
        else:
            best_cost_old = the_best_cost
        Count = 0

"""* Print the final values:"""

# Show the information to User
print('Stop at Migration :  ', Migration)
print('The number of FEs :  ', FEs)
print('The best cost     :  ', the_best_cost)
print('Solution values   :  ', the_best_value)
print('Differece         :  ', the_best_cost - func.f_global)

"""# iSOMA Python
An optimization algorithm - on python.


The iSOMA is also available in Matlab, python, and C# version, alongside some other versions like SOMA T3A and SOMA Pareto.

- Link of the article:
  - https://www.sciencedirect.com/science/article/pii/S1568494621010851
- The original version here (on MATLAB):
  - https://www.mathworks.com/matlabcentral/fileexchange/103950-the-isoma
- See more about SOMA at:
  - https://ivanzelinka.eu/somaalgorithm/Home.html

- Please cite the article if you refer to it, as follows:
  - Quoc Bao Diep, Thanh Cong Truong, Swagatam Das, and Ivan Zelinka. "Self-Organizing Migrating Algorithm with narrowing search space strategy for robot path planning." Applied Soft Computing (2021), doi: https://doi.org/10.1016/j.asoc.2021.108270

If you encounter any problems in executing these codes, please do not hesitate to contact me:
Quoc Bao Diep (diepquocbao@gmaill.com)
"""